---
title: "Homework"
author: "Zhang Chengbo"
date: "2023-12-5"
output: rmarkdown::html_vignette
Import: DAAG,bootstrap,coda,fBasics,ggplot2,lmtest,microbenchmark,stats4,boot,Rcpp
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Question

## Question1

试利用逆变换方法构造一个函数复现`sample`函数在有放回的抽样中的功能
(即参数replace=T)。

## Question2

标准Laplace分布具有如下的概率密度函数
$f(x)=\frac{1}{2}\exp\{-\left\lvert x \right\rvert \} \,,x\in\mathbb{R}$,请使用逆变换方法在该分布下
产生样本容量为1000的随机样本，并使用本章节的其中一种方法将生成的样本集与理论上给定的分布进行对比。

## Question3

请利用接受-拒绝方法来编写一个函数来产生一个样本容量为n的服从$Beta(a,b)$的随机样本，并生成一个样本容量为n的服从$Beta(3,2)$的随机样本，画出该样本的直方图并与$Beta(3,2)$的理论分布进行比较。

## Question4

重标定的*Epanechnikov*核函数具有对称的概率密度函数，$$
f_e(x)=\frac{3}{4}\left(1-x^2\right), \quad|x| \leq 1$$ *Devroye*
和*Gyorfi*给出了以下算法来模拟这种分布。我们先生成独立同分布的
$U_1, U_2, U_3\sim Uniform(-1,1)$。

如果$\left\lvert U_3 \right\rvert \geq \left\lvert U_2\right\rvert$
并且$\left\lvert U_3 \right\rvert \geq \left\lvert U_1\right\rvert$，则输出
$U_2$；否则输出 $U_3$。请编写一个函数，从 $f_e$
生成随机变量，并构建一个用于模拟随机大样本的直方图密度估计。

## Question5

证明练习 3.9(Question 4) 中给出的算法能从密度函数 $f_e$ 中生成变量。

# Answer

## Answer1

对于`sample`函数,主要参数有样本抽样选取范围$n$，样本容量$size$,
是否放回抽样$replace$,每个抽样点的抽取概率$prob$四个，我们在有放回的抽样情形下，考虑利用逆变换法构造含其它三个参数的函数`mysample`。

我们考虑先取一个随机数种子，构造一个指定样本容量$size1$的$U(0,1)$均匀分布向量，并给出该离散分布的累计分布函数，并加上0方便下一步的比较。
我们再建立两层`for`循环语句，对于$size1$个数，分别利用对应的均匀分布随机数与累计分布函数上的端点进行比较，并最终根据投入的区间位置确定输出的随机抽样取值，并将对应频率与概率进行对比。为了方便展示，之后输出的随机数数组都只展示前15个数。

```{r}
mysample=function(n1,size1,probabilty1)
{set.seed(2000)
  sample1=runif(size1)
  sdf1=cumsum(probabilty1)
  sdf2=c(0,sdf1)
   length1=length(n1)
  output=rep(1/length1, times=length1)
for(k in 1:size1)
{
  for(i in 1:length1)
  {
    if(sample1[k]<=sdf2[i+1]&&sample1[k]>sdf2[i])
    {
      output[k]=n1[i]
    }
  }
} 
  ct = as.vector(table(output))
  print(ct/sum(ct)/probabilty1)
  return(output)
}

result1=mysample(1:4,60,c(.2,.2,.3,.3))
print(result1[1:15])
result2=sample(1:4,60,replace=T,prob=c(.2,.2,.3,.3))
print(result2[1:15])
result1=mysample(1:4,1e5,c(.2,.2,.3,.3))


```

可以看到，我们的`mysample`函数在同个随机数种子下输出的结果与`sample`是不同的，可能其用了不同的随机数
在样本量较小(60)时，`mysample`函数的输出结果随机抽样的频率与
概率还有较大差距，但样本量很大($10^5$)时基本频率趋近于概率。

## Answer2

对于`Laplace`分布，考虑其累计分布函数 $$
F(x)=\begin{cases}
\frac{1}{2}e^x &\text{x < 0或x=0}\\
1-\frac{1}{2}e^{-x} &\text{x>0}
\end{cases}
$$
所以正负两边累计分布函数不相等，我们求出对应的累计分布函数的反函数(分段函数)后，利用均匀分布$Uniform(0,1)$逆变换得到所求的函数分布后，再将生成函数的密度函数与实际的概率密度进行比较。
$$
F^{-1}(u)=\begin{cases}
\ln(2u) &\text{u < 1/2或u = 1/2}\\
-\ln(2-2u) &\text{u>1/2}
\end{cases}
$$

```{r}
n=1000
output=seq(from=0,to=0,by=n)
sample2=runif(n)
for(i in 1:n)
{
  if(sample2[i]<1/2)
  {
    output[i]=log(sample2[i]*2)
  }
  if(sample2[i]>=1/2)
  {
    output[i]=-log(2-sample2[i]*2)
  }
}
print(output[1:15])
hist(output, prob = TRUE, main = expression(f(x)==0.5*e^({-abs(x)})))
y <- seq(-5, 5, .01)
lines(y, 0.5*exp({-abs(y)}))

```

可以看出确实生成的分布函数（样本容量1000）的概率密度函数与给定的`Laplace`函数基本一致。

## Answer3

本问题需要我们使用接受拒绝方法生成$Beta(a,b)$分布。
我们可以先生成$Uniform(0,1)$均匀分布(分布记为$g(x)$)的随机变量，再利用`dbeta`
函数生成服从$Bata$分布密度函数$f(x)$的随机变量。这里我们选定c=2,
如若$f(x)\geq c*g(x)$则接受这样的$x$，否则拒绝，由此
我们可得到样本容量为$n$的服从$Beta(a,b)$分布的随机变量序列。

```{r}
mybeta=function(a,b,n){
k=0
j=0
y =numeric(n)
while (k < n) {
u1 =runif(1)
j = j + 1
x = runif(1) 
if (dbeta(x,a,b)/2 > u1) {
k = k + 1
y[k]= x
}
}
return(y)
}
set.seed(10000)
beta1=mybeta(3,2,10000)
hist(beta1, prob = TRUE, main = expression(f(x)-Beta(3,2)))
y <- seq(-5, 5, .001)
lines(y, dbeta(y,3,2))
```

最终得到的所编写函数生成的满足$Beta(2,3)$分布的随机变量序列的直方图如上图所示，可以看到其密度函数与理论上的密度函数是基本一致的。

## Answer4

我们考虑先编写能生成服从密度函数$f_e$的随机变量的函数。由于该密度函数对应的累计分布函数的逆函数不易求显示解，我们在这里采用接受-拒绝法编写函数。
由于核函数分布在$[-1,1]$区间内，我们可以先生成$Uniform(-1,1)$均匀分布(分布记为$g(x)$)的随机变量，再利用密度函数$f(x)$=$f_e$生成对应的随机变量。这里我们选定c=3
，如若$f(x)\geq c*g(x)$则接受这样的$x$，否则拒绝，由此我们可得到样本容量为$n$的服从给定核函数分布的随机变量序列。

```{r}
mykernel=function(n){
k=0
y =numeric(n)
while (k < n) {
u1 =runif(1)
x = runif(1,-1,1) 
if (1/4*(1-x^2) > u1) {
k = k + 1
y[k]= x
}
}
return(y)
}
set.seed(1000)
z=mykernel(1e5)
hist(z, prob = TRUE, main = expression(f(x)==fe(x)))
y <- seq(-5, 5, .0001)
lines(y,3/4*(1-y^2) )
```

最终在得到的大样本($n=10^5$)下核函数的直方图密度估计是与理论上的核函数分布基本一致，一定程度上检验了我们编写的随机数生成函数。

## Answer5

由于$U_1, U_2, U_3\sim Uniform(-1,1)$
,那么三个随机变量$U_1, U_2, U_3$的密度函数均为$f(x)=\frac{1}{2}$,累计分布函数为$F(x)=\frac{1}{2}x+\frac{1}{2}$。我们先根据算法的流程生成大样本($n=10^5$)的随机变量序列，并画出直方图得到一个相应的密度估计，并与理论的核函数密度进行比较。

```{r}
algkernel=function(n){
k=0
y =numeric(n)
while (k < n) {
x1 =runif(1,-1,1)
x2 = runif(1,-1,1) 
x3=runif(1,-1,1)
if (abs(x3) > abs(x2)&&abs(x3)>abs(x1)) {
k = k + 1
y[k]= x2
}
else
{k = k + 1
y[k]= x3}
}
return(y)
}
l=algkernel(1e5)
hist(l, prob = TRUE, main = expression(f(x)==fe(x)))
y <- seq(-5, 5, .0001)
lines(y,3/4*(1-y^2) )
```

我们也可以看出直方图密度估计是与理论上的核函数分布基本一致，一定程度上验证了该算法的有效性。
# Question

## Question1

对于*Buffon*投针试验圆周率的参数估计结果，请证明$\rho=\frac{l}{d}$能使得$\hat{\pi}$的渐近方差达到最小(采用$\delta$方法，$m\sim B(n,p)$),记最优的$\rho$为$\rho_{min}$.

## Question2

取三个不同的$\rho$($0\leq\rho\leq 1$),其中一个为$\rho_{min}$

1.若$\rho_{min}=1$，则取其它两个$\rho$为0.5,0.8.

2.若$\rho_{min}<1$，则取其它两个$\rho$为$\frac{\rho_{min}}{2}$,1.

再用$MontoCarlo$方法比较在三个$\rho$之下$\hat{\pi}$的方差大小(取$n=10^6,$模拟重复的次数K=100)

## Question3

对于例子5.6中描述的利用控制变量法进行处理的$Monto Carlo$积分
$$ \theta=\int_0^1 e^x dx $$我们现在考虑对偶变量方法，计算$Cov(e^U,e^{1-U})$以及$Var(e^U+e^{1-U})$,
其中$U\sim Uniform(0,1)$。那么利用对偶变量方法得到的估计量$\hat{\theta}$的方差相比一般的$MontoCarlo$方法来说降低了多少？

## Question4

使用对偶变量方法与普通$MontoCarlo$方法进行数值模拟估计$\theta$。计算使用对偶变量方法减少方差的百分比的经验估计。将该结果与练习5.6中的理论值进行比较

# Answer

## Answer1

我们对于该*Buffon*投针试验，考虑得到其$Monto Carlo$估计的方差具体表达形式。由于投针n次m次投中线上的这一事件发生的概率服从二项分布$B(n,p)$,理论上投中的概率为
$P\left(\frac{l}{2} \sin (Y) \geq X\right)=\frac{2 l}{d \pi}$,其中
$Y \sim U(0, \pi / 2), X \sim U(0, d / 2)$。$\hat{\pi}$估计量的取值为$$\hat{\pi}=\frac{2l}{d *P\left(\frac{l}{2} \sin (Y) \geq X\right)}$$

记事件A为单次投针结果投到线上。

那么$$Var(\frac{m}{n})=\frac{p(A)(1-P(A))}{n}=\frac{\frac{2\rho}{\pi  }(1-\frac{2\rho}{\pi  })}{n}$$

实际上$Monto Carlo$模拟计算得到的$\hat{P}\left(\frac{l}{2} \sin (Y) \geq X\right)=\frac{m}{n}$。

那么根据$\delta$方法有
$$Var(\hat{\pi})=Var(\frac{m}{n})*(\frac{\pi^2}{2\rho})^2$$

代入$var(\frac{m}{n})$并经过对该函数求导处理可得当$\rho=1$的时候方差取到极小值，结合$\rho\geq0$我们也能判断$\rho=1$也是圆周率估计量渐近方差的最小值。

## Answer2

我们根据题意进行数的选取并比较它们的方差,由于先选三个[0,1]中的随机数构成初始的三个$\rho$,再比较三个$\rho$从而得到使得圆周率的渐近方差（$n=1e5$）达到最小的数$\rho_{min}$,选出最小数后将$min\{1,2*\rho_{min}\},\rho_{min},\rho_{min}/2$作为三个新的$\rho$继续比较。
如此循环K=100次得出最终三个$\rho$结果。

```{r}
set.seed(10000)
pihat=seq(from=1e-4,to=1,by=1e-4)
data1=runif(3)
result=numeric(length=3)
for (i in 1:100)
{i
for(j in 1:3)
{l=data1[j]
d=1
for(k in 1:1e4)
{X =runif(100,0,d/2)
Y =runif(100,0,pi/2)
  pihat[k]= 2*l/d/mean(l/2*sin(Y)>X)}
  result[j]=var(pihat)
  m=which(result==min(result),arr.ind =F)
min1=data1[m]
data1=c(min1*0.5,min1,min(c(2*min1,1)))
}

}
print(data1)

```
最终循环得到的三元组中使得渐近方差最小的$\rho=1$,由此我们可以得到在$\rho\in[0,1]$时，当$\rho=1$时圆周率的渐近方差（$n=1e5$）可以达到最小$\rho_{min}$。
## Answer3

本问题需要我们先计算$Cov(e^U,e^{1-U})$以及$Var(e^U+e^{1-U})$,
其中$U\sim Uniform(0,1)$，随机变量$U$的累积分布函数为$F(x)=x$,那么根据随机变量的变换，随机变量$e^U$的累积分布函数为$F(u_1)=ln(u_1)$,随机变量$e^{1-U}$的累积分布函数为$F(u_2)=ln(u_2)$，那么可以计算得

$$
\begin{aligned}
Cov(e^U,e^{1-U})&=E(e^U*e^{1-U})-E(e^U)E(e^{1-U})=e-(e-1)^2=-e^2+3e-1=-0.23<0\\
Var(e^U+e^{1-U})&=Var(e^u)+Var(e^{1-u})+2Cov(e^u,e^{1-u})=-3e^2+10e-5=0.016
\end{aligned}
$$
那么随机变量的$e^U$与$e^{1-U}$的协方差小于0，两者和的方差小于方差之和，那么在理论上利用对偶方法确实能够有效降低$MontoCarlo$模拟得到的估计量方差。且理论上对偶方法降低方差的百分比$A$为
$$
A=\frac{{Var(e^u)}-Var(\frac{e^U+e^{1-U}}{2})}{{Var(e^u)}}*100\%=98.4\%
$$

最终得到的对偶方法生成的估计量降低方差的百分比$A$为98.4%

## Answer4

我们考虑利用一般$MontoCarlo$估计与对偶方法的$MontoCarlo$估计在大样本下分别对参数$\theta$进行估计得到两个估计量$\hat{\theta}$的经验方差，从而进行一定的比较。

```{r}
set.seed(2000)
num=1e5
n1=runif(num)
result1=exp(n1)
mean1=mean(result1)
var1=var(result1)
print(c(mean1,var1))
n2=runif(num/2)
result2=c(0.5*exp(n2)+0.5*exp({1-n2}))
mean2=mean(result2)
var2=var(result2)
print(c(mean2,var2))
stand=exp(1)-1
A=(var1-var2)/var1
print(A)
```

最终在得到的大样本($n=10^5$)下对偶方法生成的估计量$\hat{\theta}$均值与一般方法生成估计量的均值相当接近，而对偶方法相当一般$MontoCarlo$估计方法降低方差的百分比$A$为98.4%,与理论结果比较接近。
# Question

## Question1

$\operatorname{Var}\left(\hat{\theta}^M\right)=\frac{1}{M k} \sum_{i=1}^k \sigma_i^2+\operatorname{Var}\left(\theta_I\right)=\operatorname{Var}\left(\hat{\theta}^S\right)+\operatorname{Var}\left(\theta_I\right)$,
此时
$\theta_i=E[g(U) \mid I=i], \sigma_i^2=\operatorname{Var}[g(U) \mid I=i]$
且 $I$ 服从集合 $\{1, \ldots, k\}$上的均匀分布.

证明如果$g$ 是一个区间 $(a, b)$上的连续函数, 那么
$\operatorname{Var}\left(\hat{\theta}^S\right) / \operatorname{Var}\left(\hat{\theta}^M\right) \rightarrow 0$
当 $b_i-a_i \rightarrow 0$ 对所有的 $i=1, \ldots, k$都成立.

## Question2

请找到定义在$(1, \infty)$两个重要性函数 $f_1$ 以及 $f_2$ 而且与$g(x)$
比较接近。 $$
g(x)=\frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2}, \quad x>1 .
$$
这两个重要性函数中哪一个在通过重要性采样方法进行MC估计的时候具有较小的方差?
$$
\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x
$$ 解释你做出判断的原因.

## Question3

通过重要性采样的方法得到以下函数的MC估计结果。 $$
\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x
$$

## Question4

利用分层重要性采样方法得到 例子 5.13中的估计结果
且将其与例子5.10的结果进行对比.

## Question5

假定我们用 95%的对称 t
区间在样本数据是非正态的情况下来估计平均数。那么置信区间覆盖均值的概率不一定等于
0.95.现在使用MC方法估算样本容量为 $n = 20$
的服从$\chi^2(2)$分布的随机样本的t区间覆盖概率与例子6.4中的结果进行对比。

## Question6

使用MC模拟来研究在抽样总体不服从非正态分布的情况下，t
检验的经验第一类错误率是否近似等于名义显著性水平$\alpha$。t
检验在轻微偏离正态性的情形下是稳健的。讨论采样总体 (i) $\chi^2(1)$、(ii)
$Uniform(0,2)$ 和 (iii) $Exponential$(rate=1)
时的模拟结果。在每种情况下，分别检验 $H_0 : µ = µ_0$ 与
$H0 : µ \neq µ_0$，其中 $µ_0$ 是 $\chi^2(1)、Uniform(0,2)$和
$Exponential(1)$的均值。

# Answer

## Answer1

相当于证$\frac{\operatorname{Var}\left(\hat{\theta}^S\right)}{\frac{1}{M}\operatorname{Var}\left(\theta_I\right)}=\frac{\frac{1}{K}\sum_1^k\sigma_i^2}{\operatorname{Var}\left(\theta_I\right)}\rightarrow 0$
当$b_i-a_i\rightarrow 0$时，我们可以得到$k\rightarrow\infty$。 而 $$
\begin{aligned}
E(\theta_i)=&E(g(U)),E^2(\theta_i)=E^2(E(g(U))=(Eg(U))^2\\
\operatorname{Var}(\theta_I)=&E(\theta_i^2)-E^2(\theta_i)=\\
-E(Var(g(U|I=i)))+&E(E(g^2(U)|I=i))-(Eg(U))=Var(g(U))-\frac{1}{k}\sum_{i=1}^k\sigma_i^2\end{aligned}
$$
而由g连续,${Var(g(U))}\geq\sum_{i=1}^k\sigma_i^2=\sum_{i=1}^kVar(g(U)|I=i)$,有$\frac{Var(\theta_I)}{\operatorname{Var}\left(\hat{\theta}^S\right)}\rightarrow\infty$当$k\rightarrow \infty$时，原命题得证。

## Answer2

我们考虑以下两个重要性函数 $f_1$ 以及 $f_2$ $$
f_1=\frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}*\frac{1}{1-\Phi(1)},f_2=e^{-(x-1)}
$$

其中$\Phi(x)$为标准正态分布函数对应的累计分布函数CDF。即$f_1$为
在区间$(1,\infty)$中截取的正态分布密度函数，$f_2$为参数$\lambda=1$对应的指数分布密度函数的自变量向右平移一单位得到的密度函数。那么$\frac{f}{f_i},i=1,2$可以表示为以下形式
$$
\frac{g}{f_1}=x^2*{1-\Phi(1)},\frac{g}{f_2}=\frac{1}{\sqrt{2\pi}}x^2e^{\frac{-x^2}{2}+(x-1)}
$$

之后我们可以计算出我们以下通过模拟的方法计算在两个重要性函数对应的两种重要性采样中MC估计的方差结果。我们先通过函数得到服从正态分布与指数分布的随机数，再利用直方图对生成的随机样本服从指定分布这一事实进行验证，最后利用期望的计算方法依次得到积分的估计量与估计量方差结果。

```{r}
set.seed(2000)
m=10^5
sample1=numeric(m)
for (i in 1:m)
{
  sample1[i]=rnorm(1)
  while(sample1[i]<=1)
  {
    sample1[i]=rnorm(1)
  }
}
sample2=rexp(m,rate=1)+1
#两个函数的形式
hist(sample1,prob=T)
y <- seq(1, 100, .001)
lines(y, dnorm(y)/(1-pnorm(1)))
hist(sample2,prob=T)
y <- seq(1, 100, .001)#验证分布服从指定分布的结果
lines(y, dexp(y,rate=1)*exp(1))
result1<-result2<-numeric(m)

for (i in 1:m)
{result1[i]=sample1[i]^2*(1-pnorm(1))
result2[i]=sample2[i]^2/sqrt(2*pi)*exp(-sample2[i]^2/2+sample2[i]-1)
}


print(c(mean(result1),mean(result2)))
print(c(var(result1),var(result2)))
```

在大样本($m=10^5$)下，我们得到输出的估计量均值与方差，其中第一种是截断的正态分布，第二种为平移的指数分布，我们可以看出两种重要性函数得到的估计量取值均为0.400左右，但平移的指数分布作为重要性函数得到的估计量的方差明显小于第一种截断的正态分布分布。

## Answer3

根据Answer2结果我们可以得到所求积分的MC估计量为0.400

## Answer4

在该例子5.13中，原题中给出概率密度函数有点问题，其在每个子区间$(\frac{j-1}{5},\frac{j}{5}]$的积分都不等于1，为了使得该题目能够正常解决，我们对于密度函数的系数进行一些改变，新的子区间上的概率密度函数为
$$
P(x)=\frac{e^{-x}}{\int_{\frac{j-1}{5}}^{\frac{j}{5}}e^{-x}}
=\frac{e^{-x}}{(e^{-\frac{j-1}{5}}-e^{-\frac{j}{5}})}\quad x\in(\frac{j-1}{5},\frac{j}{5}]
$$
那么我们以下根据该密度函数进行密度函数的重要性函数的分层抽样，在分层的情况中我们考虑在每个子区间中抽样频率相同的情形。具体步骤我们先利用逆变换法生成随机数，代入利用重要性采样方法计算出每一个小区间的积分，再进行加总求和。

```{r}
set.seed(1000)
m=10^4
n=5
N=50
sample1=sample2=numeric(m)
est1=matrix(0,nrow=N,ncol=2)
est2=matrix(0,nrow=N,ncol=2)
T=numeric(n)

for(i in 1:N)
{g <- function(x) {
exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
  sample1=runif(m)#逆变换法
x = -log(1 - sample1* (1 - exp(-1)))
fg <- g(x) / (exp(-x) /(1 - exp(-1)))
est1[i,1] <- mean(fg)
#Example6.10结果


for (j in 1:n)
  {g <- function(x) {
exp(-x - log(1+x^2)) * (x >(j-1)/5) * (x < ((j)/5))
}
  sample2=runif(m/n,0,1)
  x=-log(exp(-(j-1)/5)-sample2 * (exp(-(j-1)/5) - exp(-(j)/5)))
fg1 <- g(x) / (exp(-x) / (exp(-(j-1)/5) - exp(-(j)/5)))
T[j]=mean(fg1)
#新方法分层抽样
}
est1[i,2] <- sum(T)
#分层重要性抽样结果
}
print(round(apply(est1,2,mean),4))
print(round(apply(est1,2,sd),4))
```

我们通过新方法也就是在每个小区间使用分层重要性抽样得到的结果进行MC估计的估计量的结果与一般重要性采样方法大致接近，均为0.525，但标准差结果明显比一般方法好。

## Answer5

我们可以利用t分布的相关分位数性质得到生成的t区间,对于服从$\chi^2(2)$分布的随机变量$X$，其均值为2，方差为2n,所以我们可以如下构建t统计量,我们用样本标准差$S^2$代替总体标准差
$$
t=\frac{\overline{X}-2}{\sqrt{\frac{1}{n}s^2}}\sim t(n-1)
$$
那么可以得到均值置信区间$[2-t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}},2+t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}}]$.

```{r}
set.seed(200)
n=20
m=10^5
num1=num2=0
for(i in 1:m){
x <- rchisq(n,df=2)
samplesd=sd(x)
meanx=mean(x)
leftpoint=2-(qt(0.975,df=19)*sd(x)/sqrt(20))
rightpoint=2+(qt(0.975,df=19)*sd(x)/sqrt(20))#t分布
leftpoint1=2-(qnorm(0.975)*2/sqrt(20))
rightpoint1=2+(qnorm(0.975)*2/sqrt(20))#正态
if(leftpoint<meanx && meanx<rightpoint)
{
  num1=num1+1
}
if(leftpoint1<meanx && meanx<rightpoint1)
{
  num2=num2+1
}

}
result1=num1/m
result2=num2/m
print(c(result1,result2))

```

得到的t区间覆盖率为0.919显著低于0.95，说明此时CI是比较自由的。而
此时正态区间的覆盖率比较接近0.95。

## Answer6

我们分别在总体为$\chi^2(1)$、 $Uniform(0,2)$ 和
$Exponential$(rate=1)时计算三种分布下的第一类经验理论值结果。同样根据t分布性质检验统计量$$
t=\frac{\overline{X}-\mu}{\sqrt{\frac{1}{n}s^2}}\sim t(n-1)
$$
那么可以得到拒绝域$[-\infty，\mu-t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}}]\bigcup[\mu+t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}},\infty]$.此时假定$\alpha=0.05$

```{r}
set.seed(200)
n=100
m=10^5
num1=num2=num3=0
for(i in 1:m){
x <- rchisq(n,df=1)
y=runif(n,0,2)
z=rexp(n,rate=1)
meanx=mean(x)
meany=mean(y)
meanz=mean(z)
leftpoint=1-(qt(0.975,df=n-1)*sd(x)/sqrt(n))
rightpoint=1+(qt(0.975,df=n-1)*sd(x)/sqrt(n))#t分布
leftpoint1=1-(qt(0.975,df=n-1)*sd(y)/sqrt(n))
rightpoint1=1+(qt(0.975,df=n-1)*sd(y)/sqrt(n))
leftpoint2=1-(qt(0.975,df=n-1)*sd(z)/sqrt(n))
rightpoint2=1+(qt(0.975,df=n-1)*sd(z)/sqrt(n))
if(leftpoint>meanx ||meanx>rightpoint)
{
  num1=num1+1
}
if(leftpoint1>meany || meany>rightpoint1)
{
  num2=num2+1
}
if(leftpoint2>meanz || meanz>rightpoint2)
{
  num3=num3+1
}

}
print(c(num1/m,num2/m,num3/m))
```

可以得到在大样本($m=10^6$)的情形下,三种总体下t检验得到的经验第一类错误的频率分别为0.0648,0.0504,0.0575，都与显著性水平$\alpha=0.05$比较接近。
# Question

## Question1

考虑$m=1000$个假设，其中前95%个原假设成立，后5%个对立假设成立。在原假设下
p值服从$U(0,1)$分布，在对立假设下，p值服从$Beta(0.1,1)$分布(可用`rbeta`生成)
应用$Bonferroni$校正以及B-H校正应用于生成的m个独立p值(应用调整p值)，得到校正后的p值，
与$\alpha$=0.1比较判断其是否拒绝原假设。基于1000次模拟可估计FWER,FDR，TPR(True
positive rate)并输出到表格里。

## Question2

假定数量服从以$\lambda$为参数的指数分布, 那么参数 $\lambda$
的MLE估计量为 $\hat{\lambda}=1 / \bar{X}$, 这里 $\bar{X}$ 为样本均值.
我们可以得出估计量 $\hat{\lambda}$的期望为
$\lambda n /(n-1)$,所以其期望的偏差为$\lambda /(n-1)$. 而估计量
$\hat{\lambda}$的 标准差是 $\lambda n /[(n-1) \sqrt{n-2}]$.
完成以下的模拟研究 来展示 bootstrap 方法的效果.

```{=tex}
\begin{itemize}
\item 真实值为 $\lambda=2$.
\item 样本容量分为以下三种情形 $n=5,10,20$.
\item Bootstrap 方法重复次数为 $B=1000$.
\item 循环的重复次数为 $m=1000$ 次.
\item 比较 bootstrap 偏差和 bootstrap 标准误差的平均值与理论值，并对结果做出评价
\end{itemize}
```

请求得例 7.2 中相关统计量的 bootstrap 方法t 置信区间估计值 (law data in
bootstrap)

# Answer

## Answer1

我们先取$m=1000$个样本构成的数据集，其中$950$个样本服从$U(0,1)$，$50$个样本服从给定的
$Beta$分布。我们分别利用$Bonferroni$方法与$BH$方法输出调整后的p值结果。

```{r}
set.seed(1000)
N=1000
alpha=0.1
num1=num2=num3=num4=numeric(N)
for(k in 1:1000)
{
sample1=runif(950)
sample2=rbeta(50,shape1=0.1,shape2=1)
position1=rep(0,times=950)
position2=rep(1,times=50)
position=c(position1,position2)#位置保留原假设与备择假设的分组信息
sample0=c(sample1,sample2)
sampleall=(cbind(sample0,position))#样本生成
n=1000
bonferroni1=BH1=numeric(n)
sample0=sort(sample0)
sampleall=sampleall[order(sampleall[,1]),]#样本与位置信息一起排序
for (i in 1:n)
{bonferroni1[i]=n*sample0[i]#bonferroni方法修正
  if( bonferroni1[i]>1)
  {
    bonferroni1[i]=1
  }
}
BH1=sample0*n/(1:n)#BH方法修正
for (i in 1:(n-1))
{
  if( BH1[i]>BH1[i+1])
  {
    BH1[i]=BH1[i+1]
  }
 if( BH1[i]<0.1&&BH1[i+1]>0.1)
  {
    num2[k]=i
    num4[k]=sum(sampleall[1:i,2])#BH方法拒绝域中备择假设个数
 }
   if( bonferroni1[i]<0.1&&bonferroni1[i+1]>0.1)
  {
    num1[k]=i#所有落入拒绝域的个数
    num3[k]=sum(sampleall[1:i,2])#bonferroni方法拒绝域中备择假设个数
  }
}

}
size1=size2=0
for(j in 1:N)
{
  if(num3[j]/num1[j]<1)
  {size1=size1+1}
   if(num4[j]/num2[j]<1)
  {size2=size2+1}
}
mean1=size1/N
mean2=size2/N
mean3=mean(1-num3/num1)
mean4=mean(1-num4/num2)
mean5=mean(num3/50)
mean6=mean(num4/50)
print(c(mean1,mean2,mean3,mean4,mean5,mean6))#输出三个指标在两种方法下的估计值
```

我们这里设定显著性水平为10%是比较合理的。我们进行$M$=1000次实验后分别得出落入拒绝域的原假设与备择假设下个数，通过求均值得到三个指标的估计值。
根据$Bonferroni,BH$两种方法得到的三个统计指标估计值如下表所示。

|              | FWER  | FDR     | TPR   |
|--------------|-------|---------|-------|
| $Bonferroni$ | 0.1   | 0.00512 | 0.399 |
| $BH$         | 0.928 | 0.094   | 0.562 |

## Answer2

我们先根据题意分别生成样本容量为$n=5,10,21$的指数分布样本，并进行依次的$Bootstrap$估计，在
1000次重复次数下得到偏差与标准误差的估计值。

```{r}
set.seed(2000)
length1=5
length2=10
length3=20
m=B=1000
lambda=2
result1=result2=result3=numeric(B)
mean1=mean2=mean3=numeric(m)
sd1=sd2=sd3=numeric(m)#三种样本容量的数据
for(k in 1:m)
{sample1=rexp(length1,rate=lambda)
  sample2=rexp(length2,rate=lambda)
  sample3=rexp(length3,rate=lambda)
for(i in 1:(B))
{ 
  sample1star=sample(sample1,length1,replace = TRUE)
  result1[i]=1/mean(sample1star)
 sample2star=sample(sample2,length2,replace = TRUE)
   result2[i]=1/mean(sample2star)
 sample3star=sample(sample3,length3,replace = TRUE)
   result3[i]=1/mean(sample3star)
}
mean1[k]=mean(result1)-1/mean(sample1)
mean2[k]=mean(result2)-1/mean(sample2)
mean3[k]=mean(result3)-1/mean(sample3)
sd1[k]=sd(result1)
sd2[k]=sd(result2)
sd3[k]=sd(result3)
}
meansd1=mean(sd1)
meansd2=mean(sd2)
meansd3=mean(sd3)
meanbias1=mean(mean1)
meanbias2=mean(mean2)
meanbias3=mean(mean3)#1000次模拟的均值与偏差结果
print(c(meanbias1,meanbias2,meanbias3))
print(c(meansd1,meansd2,meansd3))
```

在重复次数($m=10^3$)下，我们得到输出的$Bootstrap$方法估计量偏差与标准差的估计值如下，并与题中给出的偏差与标准差的理论值进行对比。

| sample size | sd(theoretical) | sd(simulation) | bias(theoretical) | bias(simulation) |
|---------------|---------------|---------------|---------------|---------------|
| 5           | 1.443           | 2.231          | 0.5               | 0.602            |
| 10          | 0.786           | 0.815          | 0.222             | 0.218            |
| 20          | 0.496           | 0.499          | 0.105             | 0.103            |

: Theoretical and simulation value of standard error and bias

可以发现在样本量较大时标准差的理论值与真实值之间均比较接近，样本量较小时标准差的理论值与真实值偏差较大,对于均值也有类似结果。


# Question

## Question1

参考练习7.4，请用标准正态方法、基本方法、百分位方法和 BCa
方法计算平均故障间隔时间 $\frac{1}{\lambda}$ 的 95%
Bootstrap方法置信区间，并解释它们输出结果不同的原因。

## Question2

请参考练习 7.7。求偏差和标准误差的Jackknife方法的偏差和标准误差估计值。

## Question3

在例 7.18 中，我们使用了 "留一法"（n 折）交叉验证来选择最佳拟合模型。
请使用两两交叉验证(留二法)来比较并拟合最优模型。

# Answer

## Answer1

由指数分布的知识我们可以得到，样本均值$\mathbb{E}X$是$\frac{1}{\lambda}$的MLE估计。我们可以
据此利用boot包中的构建

```{r}
set.seed(1000)
library(boot)
attach(aircondit)
m=1e3
data1=c(3, 5, 7, 18, 43, 85, 91, 98,100, 130, 230, 487)
boot.mean <- function(x,i) {mean(x[i])}
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2)
for(i in 1:m){
de <- boot(data=data1,statistic=boot.mean ,R = 999)
ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
ci.perc[i,]<-ci$percent[4:5];ci.bca[i,]<-ci$bca[4:5]
}
cat('norm :',c(mean(ci.norm[,1]) ,mean( ci.norm[,2])))
cat('basic :',c(mean(ci.basic[,1]) ,mean( ci.basic[,2])))
cat('perc :',c(mean(ci.perc[,1]) ,mean( ci.perc[,2])))
cat('BCa :',c(mean(ci.bca[,1]) ,mean( ci.bca[,2])))
```

我们可以得到四种方法的置信区间结果，可以看到Bca方法无论是在置信区间的左端点还是右端点都在四种方法最大，可能受到了比较大的偏差校正作用影响。而标准正态置信区间和百分位数置信区间相对在四种方法中
置信区间既不偏大也不偏大，基本方法受较大的数据值的影响较小，置信区间的左端点与右端点在四种方法中都偏小。

## Answer2

根据练习7.7，我们需要对于5\*5的得分数据样本的协方差阵计算其最大特征值所占所有特征值之和比例的
Jackknife估计值。其中估计量的形式如下表述：

$$
\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{j=1}^5 \hat{\lambda}_j}
$$ 我们给出如下的算法过程

Input:维数为88\*5的得分数据score data

1.  每次减少一列1\*5的样本，重复88次。再通过剩下84\*5的样本的协方差矩阵计算出总体协方差阵的MLE估计$\hat{\Sigma}_i(i=1\ldots88)$

2.  利用$\hat{\Sigma}_i$计算出五个特征值的估计值$\hat{\lambda_{1,i}},\ldots\hat{\lambda_{5,i}}$,并得到第i个$\hat{\theta_i}=\frac{\hat{\lambda}_{1,i}}{\sum_{j=1}^5 \hat{\lambda}_{j,i}}$估计量的值

3.  最后利用Jackknife对应的偏差与标准差公式进行计算即可
Output:对应的偏差与标准差的估计值

```{r}
set.seed(1000)
library(bootstrap)
attach(scor)
sample1=scor
realcov=cov(sample1)
realeigresult=eigen(realcov)
realtheta=realeigresult$values[1]/sum(realeigresult$values)
realtheta
num=nrow(sample1)
hattheta=numeric(num)
for(i in 1:num)
{
  hatsigma=cov(sample1[-i,])
  eigresult=eigen(hatsigma)
  hattheta[i]=eigresult$values[1]/sum(eigresult$values)
}
bias=(num-1)*(mean(hattheta)-realtheta)
se=sqrt((num-1)/num*sum((hattheta-(rep(1,times=num)*mean(hattheta)))^2))
```

由此我们可以得到Jackknife估计该估计量$\hat{\theta}$的偏差为$0.00107=1.07*10^{-3}$,标准差为0.0496

## Answer3

此时我们需要使用两两交叉验证的方法来比较与选择最优模型。我们可以先从ironslag数据集抽取的53个样本
中利用随机抽样的方法选取出两个样本作为验证集，剩下51个样本作为测试集.在测试集上拟合模型，并在
测试集上进行验证，获得两个回归的样本残差值并计算残差平方和进行比较，模拟重复次数为N=1000次。

```{r}
set.seed(1000)
library(DAAG)
attach(ironslag)
n =length(magnetic) #in DAAG ironslag
e1 = e2 = e3 = e4=numeric(1000)
num=0
# for n-fold cross validation
# fit models on leave-two-out samples

for(i in 1:(1000)){
  sample1=sample(1:n,size=2,replace=FALSE)
  sample1=sort(sample1)
  j=sample1[1]
  k=sample1[2]
    y=magnetic[-j]
    y=y[-k]
    x = chemical[-j]
    x=x[-k]
    num=num+1
    J1 =lm(y ~ x)
    yhat1 = J1$coef[1] + J1$coef[2] * c(chemical[k],chemical[j])
    e1[i]=(c(magnetic[k],magnetic[j]) - yhat1)[1]^2
    +(c(magnetic[k],magnetic[j]) - yhat1)[2]^2
    J2 = lm(y ~ x + I(x^2))
    yhat2 = c(J2$coef[1] + J2$coef[2] * chemical[k] 
              +J2$coef[3] * chemical[k]^2,
   J2$coef[1] + J2$coef[2] * chemical[j] 
   +J2$coef[3] * chemical[j]^2)
    e2[i] = (c(magnetic[k],magnetic[j]) - yhat2)[1]^2
    +(c(magnetic[k],magnetic[j]) - yhat2)[2]^2
    J3 =lm(log(y) ~ x)
    logyhat3 = J3$coef[1] + J3$coef[2] * chemical[k]
    otherlogyhat3 = J3$coef[1] + J3$coef[2] * chemical[j]
    yhat3 = c(exp(logyhat3),exp(otherlogyhat3))
    e3[i] = (c(magnetic[k],magnetic[j]) - yhat3)[1]^2
    +(c(magnetic[k],magnetic[j]) - yhat3)[2]^2
    J4 = lm(log(y) ~ log(x))
    logyhat4 =J4$coef[1] + J4$coef[2] * log(chemical[k])
    otherlogyhat4 =J4$coef[1] + J4$coef[2] * log(chemical[j])
    yhat4 = c(exp(logyhat3),exp(otherlogyhat4))
    e4[i] =(c(magnetic[k],magnetic[j]) - yhat4)[1]^2
    +(c(magnetic[k],magnetic[j]) - yhat4)[2]^2
  }
print(c( c(mean(e1), mean(e2), mean(e3), mean(e4))))
```

那么我们可以得到第二个模型在leave-two-out方法下拟合原数据的残差平方和达到最小，也就是选用二次
函数形式拟合效果最好。 
# Question

## Question1

请参考离散分布样本情形的Metropolis-Hastings采样算法的平稳性证明，证明平稳性质在连续分布样本的情形下也成立

## Question2

利用检验同分布的置换检验来完成双样本的 Cramer-von Mises 检验，并应用在例
8.1 和例 8.2 中的数据中。

## Question3

第 6.4 节中的Count 5同方差检验是基于极值点的最大数目建立的 。例 6.15
表明Count
5标准不适用于样本量不等的情况。请构建一个基于最大极值点数目的同方差置换检验，使得该检验能在样本量不相等的情况下也适用。

# Answer

## Answer1

在连续样本的情形下给出的提议分布记为$g(r \mid s)$，对应的概率密度分布函数为$f(x)$，
每次循环得到的接受概率为
$\alpha(s, r)=\min \left\{\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)}, 1\right\}$.
此时转移概率密度的形式如下 $$
K(r, s)=\alpha(r, s) g(s \mid r)+I(s=r)\left[1-\int \alpha(r, s) g(s \mid r)\right] \text {. }
$$

我们想要证明平稳性，即需要证明$K(s, r) f(s)=K(r, s) f(r)$.

由于$\alpha(s, r)=\min \left\{\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)}, 1\right\}$，$\alpha(r, s)=\min \left\{\frac{f(s) g(r \mid s)}{f(r) g(s \mid r)}, 1\right\}$,所以当
$\alpha(s,r)<1$时，$\frac{f(r) g(s \mid r)}{f(s) g(r \mid s)}$\<1,此时$\frac{f(s) g(r \mid s)}{f(r) g(s \mid r)}$\>1,$\alpha(r,s)=1$，反之若$\alpha(r,s)<1$也能得到$\alpha(s,r)=1$。代入计算得到：
$$ k(s,r)f(s)=\{
\begin{array}{c}
 g(r|s)f(s)+I(s=r)[f(s)-\int g(r|s)f(s)] \quad if \quad\alpha(s,r)=1 \\
g(s|r)f(r)+I(s=r)[f(r)-\int g(s|r)f(r)] \quad if \quad\alpha(s,r)<1
\end{array}
$$

$$ k(r,s)f(r)=\{
\begin{array}{c}
 g(r|s)f(s)+I(s=r)[f(s)-\int g(r|s)f(s)] \quad if \quad\alpha(r,s)<1 \\
g(s|r)f(r)+I(s=r)[f(r)-\int g(s|r)f(r)] \quad if \quad\alpha(r,s)=1
\end{array}
$$

\$\$当$\alpha(s,r)=1$时，我们可以得到$\alpha(r,s)=1$或$\alpha(r,s)<1$，当$\alpha(r,s)<1$时显然两者相等，而当$\alpha(r,s)=1$时，我们有$g(r|s)f(s)=g(s|r)f(r)$，$I(s=r)[f(s)-\int g(r|s)f(s)]=I(s=r)[f(r)-\int g(s\|r)f(r)]$，等式仍然成立。
当$\alpha(s,r)<1$时,我们可以得到$\alpha(r,s)=1$,平稳性成立。

## Answer2

我们给出如下的算法过程：对于给定的样本$X=(x_1,x_2,\ldots x_n),y=(y_1,y_2,\ldots y_m)$,
我们先根据样本计算出分别依赖于样本$X,Y$的两个经验分布函数$F_n(x)$与$G_m(y)$，
再利用置换检验将样本$X,Y$合并后随机重复$N=10^4$次采样，根据统计量的具体形式得出
Cramer-von Mises在原假设下的大样本近似零分布，再根据样本的取值计算出
Cramer-von Mises检验统计量的值。
根据样本的检验统计量以及检验的分布计算出检验p值。建立相应的函数后再代入两个例子中的该组数据进行计算应用。

```{r}
set.seed(1000)
library(bootstrap)
myfunction=function(x,y)#建立函数
{
  numx=length(x)
  numy=length(y)
  num=numx+numy#计算样本的维数
  z=c(x,y)
  R=10^4
  result=numeric(R)
  power=0
  for(i in 1:R)
  {
    sample1=sample(z,num,replace=FALSE)
    pro1=sample1[1:numx]
    pro2=sample1[numx+1:num]
   fun1=ecdf(pro1)
    fun2=ecdf(pro2)#获取经验分布函数
    result1=fun1(z)
    result2=fun2(z)
    result[i]=numx*numy/((numx+numy)^2)*sum((result1-result2)^2)
    
  }
 funreal1=ecdf(x)
 funreal2=ecdf(y)
 result0=numx*numy/((numx+numy)^2)*sum((funreal1(z)-funreal2(z))^2)
 return(mean(c(result,result0)>result0))
}
attach(chickwts)
x0 <- sort(as.vector(weight[feed == "soybean"]))
y0<- sort(as.vector(weight[feed == "linseed"]))

detach(chickwts)#选取数据并代入函数
resultall=myfunction(x0,y0)
print(resultall)
```

我们可以得到，利用例子中chickwts的数据集中的数据进行两样本Cramer-von
Mises 置换检验,最终输出的检验p值为0.406，
无法说明两组数据之间确实存在显著差异。可能是该方法统计量相比t统计量更倾向于接受原假设。

## Answer3

此时我们需要建立一个在样本数量不相等的情况下也可使用的
基于最大极值点数目的双样本同方差置换检验。不妨设样本$X$的样本容量为m，样本$Y$的样本容量为n。
首先我们和之前一样对样本$X,Y$进行中心化处理，即减去自身的均值。我们再将中心化处理后的$X,Y$数据合并，在合并得到的样本容量为$m+n$的混合样本中有放回的抽取同样
容量为$m+n$的样本，划分成新的$X^{'},Y^{'}$样本，进行多次Count5检验并考虑其经验第一类错误。

```{r}
set.seed(10000)
maxout <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}
count5test <- function(x, y,N) {
x=x-mean(x)
y=y-mean(y)#中心化
z=c(x,y)
result0=numeric(N)
for(i in 1:1000){
z=sample(z,length(z),replace=FALSE)
X=z[1:length(x)]
Y=z[(length(x)+1):length(z)]
X=X-mean(X)
Y=Y-mean(Y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
result0[i]=(as.integer(max(c(outx, outy)) > 5))}
return(mean(result0))
}
n1 <-10
n2 <- 10
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1000
tests <- replicate(m, expr = {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
count5test(x, y,1000)
} )
alphahat <- mean(tests)
print(alphahat)
stat <- replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
})
print(cumsum(table(stat)) / m)#同样本容量检验
n1 <-10
n2 <- 8
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
tests <- replicate(m, expr = {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
count5test(x, y,1000)
} )
alphahat <- mean(tests)
print(alphahat)
stat <- replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
})
print(cumsum(table(stat)) / m)#异样本容量检验
```

那么我们可以得到在样本数量一致的情形下此时的经验Type I
Error=0.010与期望的经验第一类错误1-0.969=0.031非常接近。
在样本量相差不大时也有较好表现，但在样本量相差较大时表现不太好。
# Question

## Question1

考虑一个混合的Logistics回归模型$P\left(Y=1 \mid X_1, X_2, X_3\right)=\frac{\exp \left(a+b_1 X_1+b_2 X_2+b_3 X_3\right)}{1+\exp \left(a+b_1 X_1+b_2 X_2+b_3 X_3\right)}$,
此时 $X_1 \sim P(1), X_2 \sim \operatorname{Exp}(1)$ 且
$X_3 \sim B(1,0.5)$.

```{=tex}
\begin{itemize}
\item 建立一个以如下自变量作为输入值的函数 $N, b_1, b_2, b_3$ 与 $f_0$, 且输出值为 $a$. 
\item 该函数采用以下的输入值取值 $N=10^6, b_1=0, b_2=1, b_3=-1, f_0=0.1,0.01,0.001,0.0001$.

\item 画出 $-\log f_0$ 与 $a$之间的曲线图.
\end{itemize}
```
## Question2

建立随机游走形式的 Metropolis
采样方法并应用于生成标准拉普拉斯分布（见练习
3.2）。对于其中的增量，采用正态分布进行模拟。比较在使用不同方差的提议分布下生成的马氏链。同时，计算每个马氏链的接受率。

## Question3

使用Gibbs采样方法生成均值为零，标准差为单位阵,相关性为 0.9 的二元正态链
$(X_t,Y_t)$。 在舍弃一组合适的预烧样本后，绘制生成的样本图像。
利用简单线性回归模型$Y= β_0 + β_1X$ 对样本进行拟合
，并检验模型残差的正态性和方差的恒定性。

## Question4

请参考例 9.1。使用 Gelman-Rubin 方法控制马氏链的收敛性。 也可以使用 coda
[212] 软件包通过 Gelman-Rubin 方法检测马氏链的收敛性。提示： 参见 coda
函数 gelman.diag、gelman.plot、as.mcmc 和 mcmc.list 帮助完成任务。

# Answer

## Answer1

我们先如题所示设定这些参数的值，利用获得的分布信息生成样本，并将其线性组合代入Logit函数
构建逻辑回归模型。 $$
g(\alpha)=\frac{1}{N} \sum_{i=1}^N \frac{1}{1+e^{-\alpha-\beta_1 x_{1 i}-\beta_1 x_{2 i}}}-f_0
$$
我们再通过以上函数对于$P(D=1)-f_0$进行估计，求解以上方程的根并绘制出对应的曲线图。

```{r}
set.seed(1000)
N <- 1e6; b1 <- 0; b2 <- 1;b3=-1; f1 <- c(0.1,0.01,0.001,0.0001)
x1=rpois(N,lambda = 1)
x2 <- rexp(N,rate=1); x3 <- sample(0:1,N,replace=TRUE)
g <- function(alpha,f0){
tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
p <- 1/(1+tmp)
mean(p) - f0#逻辑回归部分
}
result=numeric(4)
solution1 <- uniroot(g,c(-10,0),f0=f1[1])
result[1]=round(unlist(solution1),5)[1]
solution2 <- uniroot(g,c(-10,0),f0=f1[2])
result[2]=round(unlist(solution2),5)[1]
solution3 <- uniroot(g,c(-100,0),f0=f1[3])
result[3]=round(unlist(solution3),5)[1]
solution4 <- uniroot(g,c(-100,0),f0=f1[4])
result[4]=round(unlist(solution4),5)[1]
print(result)
plot(-log(f1),result,type='l')
```

最终可以发现，随着$-log(f_0)$的取值的增加，得到的估计值$\hat{\alpha}$的不断减少，且
两者的变化近似线性关系。

## Answer2

对于`Laplace`分布，考虑其概率密度函数
$f(x)=\frac{1}{2}\exp\{-\left\lvert x \right\rvert \} \,,x\in\mathbb{R}$，累计分布函数为
$$
F(x)=\begin{cases}
\frac{1}{2}e^x &\text{x < 0或x=0}\\
1-\frac{1}{2}e^{-x} &\text{x>0}
\end{cases}
$$
那么我们考虑当提议分布选取不同方差下的正态分布时，比较生成的马氏链以及对应的接受频率。

```{r}
set.seed(1000)
library(ggplot2)
dlaplace=function(x)
{
  return(1/2*exp(-abs(x)))#Laplace分布对应的密度函数
}
rw.Metropolis <- function(n, sigma, x0, N) {
x <- numeric(N)
x[1] <- x0

u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (dlaplace(y) / dlaplace(x[i-1])))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}#Metropolis方法采样
}
return(list(x=x, k=k))
}
N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 25
rw1 <- rw.Metropolis(n, sigma[1], x0, N)
rw2 <- rw.Metropolis(n, sigma[2], x0, N)
rw3 <- rw.Metropolis(n, sigma[3], x0, N)
rw4 <- rw.Metropolis(n,sigma[4],x0,N)#四条马氏链结果
 print(c(rw1$k, rw2$k, rw3$k, rw4$k))#四种方差下的拒绝次数
 dat=cbind(rw1$x[1001:2000],rw2$x[1001:2000],rw3$x[1001:2000],rw4$x[1001:2000])
 p_line=ggplot(as.data.frame(dat))+geom_line(aes(x=1001:2000,y=dat[,1]),color='blue')+  geom_line(aes(x=1001:2000,y=dat[,2]),color='red')+  geom_line(aes(x=1001:2000,y=dat[,3]),color='yellow')+ geom_line(aes(x=1001:2000,y=dat[,4]),color='green')+labs(x= 'times',y='value')
 #方差由小到大依次是蓝红黄绿四条折线图
 

p_line
```

我们除去前1000个预烧样本，利用MCMC方法在四种不同方差下的提议分布(正态分布)构造出对应的标准拉普拉斯分布,画出相应的时序图,其中方差由小到大依次是蓝红黄绿四条折线图，
可以看到方差较大时，得到的样本分布基本在0左右波动，并且随着提议分布的方差不断增大，拒绝的频率也在不断增大。
我们可以看出在四种方差中选择方差为0.5或2为较优选择。

## Answer3

```{r}
set.seed(10000)
library(lmtest)
library(ggplot2)
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- -.8 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
###### generate the chain #####
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
X[i, 1] <- rnorm(1, m1, s1)
x1 <- X[i, 1]
m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x0 <- X[b:N, ]#生成二元正态的样本
lmmodel=lm(x0[,2]~x0[,1])
summary(lmmodel)#回归
res=lmmodel$residuals
qqnorm(res)#qq图
ks.test(res,'pnorm')#KS正态性检验
gqtest(lmmodel)#Goldfeld-Quandt检验
```

我们先通过Gibbs采样的方法迭代得到去除前1000组预烧样本的样本容量n=4000的二元正态链，并做出$Y$对于$X$的回归，
得到的回归结果我们画出残差的qq图并使用kolmogorov进行正态检验，并利用GQ检验验证同方差性，最终得到的残差具有一定
厚尾性，KS检验的结果拒绝了残差的正态性，而GQ检验的结果验证了残差的同方差性。

## Answer4

```{r}
set.seed(1000)
library(coda)
f <- function(x, sigma) {
if (any(x < 0)) return (0)
stopifnot(sigma > 0)
return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

m <- 10000
sigma <- 4
x <- numeric(m)
x[1] <- rchisq(1, df=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if (u[i] <= num/den) x[i] <- y else {
x[i] <- xt
k <- k+1 #y is rejected
}
}
set.seed(2000)
x1=numeric(m)
x1[1]=rchisq(1,df=1)
u1 <- runif(m)
for (i in 2:m) {
xt <- x1[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if(u1[i] <= num/den) x1[i] <- y else {
x1[i] <- xt
k <- k+1 #y is rejected
}
}
gelman.diag(mcmc.list(as.mcmc(x),as.mcmc(x1)),confidence = 0.95,transform = TRUE)
```

我们利用之前的MH采样方法生成了两组对应的
服从Rayleigh分布的马氏链，并用coda包中的函数gelman.diag完成收敛性的检验工作，
最终得到的下界接近于1，这也说明了所得马氏链的收敛性
# Question

## Question1

设 $x_1 \ldots x_n.i.d. Exp (\lambda)$．因为某种原因，只知道
$X_i$落在某个区间$(u_i , v_i )$，其中 $u_i<v_i$
是两个非随机的已知常数，这种数据称为区间删失数据。

(1）试分别直接极大化观测数据的似然函数与采用 EM 算法求解$\lambda$的 MLE
，证明 EM 算法收敛于观测数据的 MLE ，且具有线性的收敛速度

(2）$设(u_i , v_i ), c =1,\ldots n =10$的观测值为(11,12),(8.9), (27.28),

(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3),试分别编程实现上述两种算法以得到
$\lambda$ MLE 的数值解．

提示：观测数据的似然函数为
$L(\lambda)= \Pi_{i=1}^n P_{\lambda}( u_i\leq X_i\leq v_i )$.

## Question2

在Morra博弈中，如果从报酬矩阵的每一个条目中减去一个常数，或者将报酬矩阵的每一个条目乘以一个正常数，那么最优策略集都不会发生变化。
然而，单纯形法的最终结果可能是另一个基本可行点（也是最优点）。 计算 B
\<- A + 2，并找出博弈 B 的解，并验证它是原博弈 A 的一个极值点
(11.12)-(11.15)。 同时求出博弈 A 和博弈 B 的值。

# Answer

## Answer1

(1)我们先如题中方法计算观测数据的似然函数 为
$$L(\lambda)= P_{\lambda}( u_i\leq X_i\leq v_i )=\Pi_{i=1}^n (e^{-\lambda u_i}-e^{-\lambda v_i})$$

令其梯度为0，我们可以得到

$$
\frac{\partial log(L)}{\partial \lambda}=0\Longrightarrow \sum_{i=1}^n \frac{-u_i e^{-\lambda u_i}+v_i e^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0
$$

此时MLE的估计量$\lambda$并不存在显式的表达。

以下是我们采用EM算法求解参数$\lambda$估计问题的具体步骤。
我们需要不断使用E步与M步两个步骤对于参数$\lambda$的取值以及隐变量的条件期望进行迭代优化。

在E步中，我们主要计算给定观测数据和当前参数估计下隐变量的条件期望。
我们设定隐变量$z_i$为指示变量，表示样本 $x_i$
是否来自指数分布。在指数分布的情况下，我们可以将 $z_i$
视为一个二元变量，其条件期望为: $$
E\left(z_k \mid x_k, \lambda\right)=P\left(z_k=1 \mid x_i, \lambda\right)=\frac{(e^{-\lambda u_k}-e^{-\lambda v_k})}{\sum_{i=1}^n (e^{-\lambda u_i}-e^{-\lambda v_i})}
$$

在M步中，我们则通过最大化完整数据（包括隐变量）的对数似然函数，对于参数$\lambda$的估计进行更新。具体我们使用
${E}$ 步中计算得到的条件期望，通过最大化完整数据的对数似然函数来更新参数
$\lambda$

那么当对数似然函数为: $$
log L(\lambda)=\sum_{i=1}^n \log f\left(x_i ; \lambda\right)=\sum_{i=1}^n \log(P\left(z_k=1 \mid x_i, \lambda\right)*\lambda_{old})
$$

我们在迭代次数趋于无穷的情况下$\hat{\lambda}=\lambda_{old}$时通过对对数似然函数求偏导并令导数等于零，解出更新的
$\lambda$:\
$$
    \frac{\partial L}{\partial \lambda}=0\Longrightarrow \sum_{i=1}^n \frac{-u_i e^{-\lambda u_i}+v_i e^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0
    $$

因而我们可以知道EM 算法收敛于观测数据的 MLE

(2)\\
我们先输入以上数据，并根据(1)中说明的方法流程分别利用极大似然估计方法以及EM算法分别估计出参数$\lambda$的取值。

```{r}
set.seed(1000)

A=matrix(c(11,12,
            8,9,
           27,28,
           13,14,
           16,17,
           0,1,
           23,24,
           10,11,
           24,25,
           2,3),2,10)
A=t(A)
fx=function(x)#LogML函数
{f0=0
  for(i in 1:10)
{
  f0=f0+(-A[i,1]*exp({-x*A[i,1]})+A[i,2]*
           exp({-x*A[i,2]}))/(exp({-x*A[i,1]})-exp({-x*A[i,2]}))
}
  f0
}
out1 <- uniroot(fx,
lower = 0, upper =10)
round(c(out1$root,out1$f.root),4)#MLE估计结果展示

#E-step
E_step <- function(x, lambda) {
  p_z_given_x <- lambda * exp(-lambda * x) / (lambda * exp(-lambda * x) + lambda)
  return(p_z_given_x)
}

# M-step
M_step <- function(x, p_z_given_x) {
  lambda_new <- sum(p_z_given_x) / sum(x)
  return(lambda_new)
}

EM_algorithm <- function(x, lambda_initial, tol = 1e-6, max_iter = 1000) {
  lambda <- lambda_initial
  for (iter in 1:max_iter) {
    # E-step
    p_z_given_x <- E_step(x, lambda)
    
    # M-step
    lambda_new <- M_step(x, p_z_given_x)
    
    # Check for convergence
    if (abs(lambda_new - lambda) < tol) {
      break
    }
    
    lambda <- lambda_new
  }
  
  return(lambda)
}
result=apply(A,1,mean)
x=result
lambda_initial <- 0.05
estimated_lambda <- EM_algorithm(x, lambda_initial)
estimated_lambda 
```

最终可以发现，MLE方法得到的参数$\lambda$的估计值为0.0720,EM算法得到的参数$\lambda$为0.0289，应该是在EM算法部分的程序过程存在一定的问题。

## Answer2

在Morra博弈中我们在报酬矩阵上对于每个元素加二，并不改变之前得到的最优策略集，将改变后的新报酬矩阵代入算法进行验证。

```{r}
set.seed(1000)
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
#
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
A=A+matrix(rep(2,81),9,9)#A=A+2
library(boot) #needed for simplex function
s <- solve.game(A)
round(cbind(gameA=s$x, gameB=s$y), 7)#value of gameA and gameB
round(c(0, 0, 25/61, 0, 20/61, 0, 16/61, 0,0),7)
```

可以验证得到的gameB的解正是书中(11.15)式对应的最优解，gameA 与
gameB的值也在上述输出结果中进行了展示，均为c(0, 0, 25/61, 0, 20/61, 0,
16/61, 0,0).
# Question

## Question1

为什么我们使用unlist()函数来将列表型list数据转化为原子向量型数据，为什么as.factor()函数在此时会无法使用。

## Question2

1.dim() 应用于向量时会返回什么结果？\\ 2.如果 is.matrix(x) 为
TRUE，is.array(x) 将返回什么结果？

## Question3

1.dataframe可以有 0 行吗？那么 0 列呢？

2.当 as.matrix() 应用于具有不同类型dataframe时，会发生什么？

## Question4

下面的函数可以将向量进行缩放至 [0, 1]中去.
如何将其应用于dataframe的每一列中？
如何将它应用到dataframe的每一数值列中？

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```

## Question5

使用 vapply() 来 1. 计算数值dataframe中每一列的标准差。\\
2.计算dataframe中每一列数值的标准差。(提示：您需要使用 vapply()函数
两次）。

## Question6

考虑以下的一个二元分布， $$
f(x, y) \propto\left(\begin{array}{l}
n \\
x
\end{array}\right) y^{x+a-1}(1-y)^{n-x+b-1}, \quad x=0,1, \ldots, n, 0 \leq y \leq 1 .
$$

可以看到在固定三个变量 $a, b, n$的情形下,x,y的条件分布分别为
$\operatorname{Binomial}(n, y)$ 与 $\operatorname{Beta}(x+a, n-x+b)$.
使用 Gibbs 采样方法来生成服从目标联合概率密度$f(x, y)$的一条马氏链.
且运用microbenchmark函数比较两种语言代码的运行时间 

# Answer

## Answer1

unlist函数的主要作用是展平列表，也就是说该函数会递归地从列表中提取元素，从而得到一个一维向量型数据。
而as.vector()是函数一个通用的强制转换函数，但它并不专门设计用于处理列表，不会像unlist()那样递归提取元素，而是会保留列表结构，并不会得到真正的atomic向量。

```{r}
my_list <- list(a = 1, b = list(c = 2, d = 3), e = 4,f=list(g=5))

# 使用unlist()
result_unlist <- unlist(my_list)
print(result_unlist)


# 使用as.vector()
result_as_vector <- as.vector(my_list)
print(result_as_vector)

```

## Answer2

(1) dim() 应用于向量时会返回Null，对应维数为空

```{r}
a=c(1,1,2,3)
b=vector(length=5L)
print(dim(a))
print(dim(b))
```

(2) 如果 is.matrix(x) 为 TRUE，那么 is.array(x) 也将返回
    TRUE。因为矩阵是一个特殊的二维数组，array范围比matrix更广。

```{r}
a=matrix(2,3,3)
is.matrix(a)
is.array(a)
```

## Answer3

(1) dataframe是一种特殊的list类型的列表，可以有零行，但由于其为列表类数据，不能有零列。我们可以创建零行的dataframe，但零列的dataframe会报错。

```{r}

a<- data.frame(matrix(ncol = 5, nrow = 0))
a

```

(2) 当 as.matrix()
    应用于具有不同类型的DataFrame时，R会试着将DataFrame转换为一个矩阵。然而，由于矩阵要求所有元素具有相同的数据类型，因此在DataFrame的列具有不同的数据类型时，R会进行一些数据类型的转换，转化的次序如下：logical
    \< integer \< double \< character \<
    list，以使得整个矩阵具有统一的数据类型。

```{r}
a=data.frame(a=1,b=list(c=3,d=4))
print(as.matrix(a))
```

## Answer4

要想将该函数其应用于dataframe的每一列，只需直接运用apply函数对每一列使用该函数进行操作即可，我们同样先考虑一个只含有数值数据的dataframe。

```{r}
df1 <- data.frame(
  A = c(1, 2, 3),
  B = c(4, 5, 7)
)
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
# 使用apply()函数

result<- apply(df1,  2,scale01)
print(result)

```

而当我们只想将函数应用在其中的数值列时，我们需要先用is.numeric函数结合sapply函数对于是否是数值列进行判断，判断后再对选定的数值列使用给定函数进行相应的处理工作。

```{r}
df1 <- data.frame(
  A = c(1, 2, 3),
  B = c(4, 5, 6),
  C=c("article","subject","lesson")
)
columns <- sapply(df1, is.numeric)
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

# 使用apply()函数
result<- apply(df1[,columns], 2, scale01)
print(result)

```

## Answer5

与上一题类似地，我们只需直接运用vapply函数对每一列使用求标准差即可

```{r}
df1 <- data.frame(
  A = c(1, 2, 3),
  B = c(4, 5, 7)
)


# 使用vapply()函数
result<- vapply(df1, sd,FUN.VALUE=c(sd=0))
print(result)

```

同样地，与上一题类似地，我们需要先用is.numeric函数结合vapply函数对于是否是数值列进行判断，判断后再对选定的数值列求标准差即可

```{r}
df1 <- data.frame(
  A = c(1, 2, 3),
  B = c(4, 5, 7),
  C=c("article","subject","lesson")
)
columns <- vapply(df1, is.numeric,FUN.VALUE=c(sd=0))


# 使用apply()函数
result<- vapply(df1[,columns],  sd,FUN.VALUE=c(sd=0))
print(as.vector(result))
```

## Answer6

我们先考虑用R语言实现该过程

```{r}
library(microbenchmark)
set.seed(1000)
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
funa=function(a,b,n){


x=0
y=0.5
X[1, ] <- c(x, y)
for (i in 1:N) {
 
  x <- rbinom(1, n, y)
  
  y <- rbeta(1, x + a, n - x + b)
  
  # 存储采样结果
 X[i, ] <- c(x, y)
}
X=X[(burn+1):5000,]
return(X)
}
b=funa(1,1,10)
print(b[1:20,])

```

```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix gibbsSampler(int N, int burn, int a, int b, int n) {
  NumericMatrix X(N, 2);
  
  int x = 0;
  double y = 0.5;
  
  for (int i = 0; i < N; ++i) {
    x = R::rbinom(1, y * n);
    y = R::rbeta(x + a, (n - x) + b);
    
    X(i, 0) = x;
    X(i, 1) = y;
  }
  
  NumericMatrix result = X(Range(burn, N - 1), _);
  return result;
}

```

```{r}
library(microbenchmark)
result <- microbenchmark(funa(1,1,10), gibbsSampler(5000,1000, 1,1, 10), times = 100)
print(result)
```

我们可以看到在同样进行计算100次循环的过程中，使用一般R语言进行循环迭代所需的时间明显多于使用Rcpp所需的时间，Cpp
在循环过程中明显具有更高的效率

## 备注

``` latex
注：本次作业主要使用了一些R语言package（包括rmarkdown,rticles
,xtable等）,并在rticles的CTEX中文格式下进行编写。
```
